{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4W-BbonrAFh"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Read CSV file\n",
        "df = pd.read_csv('/content/microbes.csv')\n",
        "df\n",
        "\n",
        "#total rows and columns\n",
        "df.shape\n",
        "\n",
        "#First 5 columns and rows of dataset\n",
        "df.head()\n",
        "\n",
        "#Last 5 columns and rows of dataset\n",
        "df.tail()\n",
        "\n",
        "#Total columns\n",
        "df.columns\n",
        "\n",
        "#check null value for particular column\n",
        "df['microorganisms'].isnull()\n",
        "\n",
        "#finding total null values in columns\n",
        "df.isnull().sum()\n",
        "\n",
        "#to find null value from the dataset\n",
        "#if false = their is no null value\n",
        "#if true = their is null value \n",
        "df.isnull().values.any()\n",
        "\n",
        "#finding null value in dataset \n",
        "# Result in boolean value\n",
        "df.isnull().any\n",
        "\n",
        "#to get information of the dataset\n",
        "df.info()\n",
        "\n",
        "#to get the description of the dataset \n",
        "df.describe()\n",
        "\n",
        "#to find the unique value of whole dataset\n",
        "df.nunique()\n",
        "\n",
        "#to find the unique values of the particular column\n",
        "df.microorganisms.unique()\n",
        "\n",
        "#to count the value of particular column by groupby function\n",
        "df.groupby('microorganisms').count()\n",
        "\n",
        "#to count the value of particular column of their unique value \n",
        "df['microorganisms'].value_counts()\n",
        "\n",
        "#Visualization\n",
        "#bar graph for total value count of particular columns \n",
        "df['microorganisms'].value_counts().plot(kind='bar')\n",
        "\n",
        "#histograph\n",
        "df['microorganisms'].value_counts().plot(kind='hist')\n",
        "\n",
        "#Binning\n",
        "#histogram\n",
        "df['microorganisms'].value_counts().plot(kind='hist', stacked=True, bins=20)\n",
        "\n",
        "#Horizontal Histograph\n",
        "df['microorganisms'].value_counts().plot(kind='hist', orientation='horizontal', cumulative=True)\n",
        "\n",
        "#PieChart\n",
        "df['microorganisms'].value_counts().plot(kind='pie', autopct =\"%.2f\")\n",
        "\n",
        "#Dis Plot it only gives plot of numerical values\n",
        "sns.distplot(df['raddi'])\n",
        "\n",
        "sns.distplot(df['Area'])\n",
        "\n",
        "#BoxPlot\n",
        "sns.boxplot(df['raddi'])\n",
        "\n",
        "#CountPlot\n",
        "sns.countplot(df['microorganisms'])\n",
        "\n",
        "#to find the correlation of the dataset\n",
        "print(df.corr())\n",
        "\n",
        "#Heatmap  \n",
        "plt.figure(figsize=(24, 12))\n",
        "dataplot=sns.heatmap(df.corr(), annot=True, annot_kws={'size': 10})  \n",
        "plt.show()\n",
        "\n",
        "#to plot the correlation coeeficient on heatmap by giving the color and size of the map\n",
        "plt.figure(figsize=(24, 12))\n",
        "dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True, annot_kws={'size': 10})\n",
        "plt.show()\n",
        "\n",
        "#to get output in full array \n",
        "np.set_printoptions(edgeitems=127)\n",
        "\n",
        "#Finding outliers through graph\n",
        "#to plot particular column on boxplot and finding the outlier through boxplot\n",
        "# boxplot when outlier of single column is to be find \n",
        "sns.boxplot(x=df['Solidity'])\n",
        "\n",
        "#to plot particular column on scatter diagram and finding the outlier through scatter diagram \n",
        "#scatter diagram is when two parameters is to be compared and finding outlier\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.scatter(df['Perimeter'], df['FilledArea'])\n",
        "ax.set_xlabel('It is ratio of length of major to minor axis of an object')\n",
        "ax.set_ylabel('Number of on pixels in filledimage, returned as a scalar')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.scatter(df['raddi'], df['Area'])\n",
        "ax.set_xlabel('It is ratio of length of major to minor axis of an object')\n",
        "ax.set_ylabel('Number of on pixels in filledimage, returned as a scalar')\n",
        "plt.show()\n",
        "\n",
        "#LOG TRANSFORMATION FOR OUTLIER\n",
        "df1= x\n",
        "df1.boxplot(column= 'raddi')\n",
        "\n",
        "sns.distplot(df1)\n",
        "\n",
        "#here we have few zero values during input so we get negative as values as output in transformed data\n",
        "log_df = x\n",
        "out_log_df = np.log10(log_df)\n",
        "out_log_df\n",
        "\n",
        "# here the number of outliers have increased bcoz we had zero values in our input and LOG transformstion is not useful for zero values as inputs \n",
        "out_log_df.boxplot(column= 'raddi')\n",
        "\n",
        "# here we are not able to plot dist plot as here we have negative values which will not be accepted under float data type\n",
        "sns.distplot(out_log_df)\n",
        "\n",
        "# SQUARE ROOT TRANSFORMATION FOR OUTLIER\n",
        "\n",
        "df1 = x\n",
        "df1.boxplot(column = 'raddi')\n",
        "plt.show()\n",
        "\n",
        "sns.distplot(df1)\n",
        "\n",
        "square_out = np.sqrt(df1)\n",
        "square_out\n",
        "\n",
        "#EARLIER THERE WERE 3 Outliers while now there are only two  outliers now so 1 outlier is  treated with this method \n",
        "square_out.boxplot(column = 'raddi')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Statisical way of finding Outliers \n",
        "# -> Z-Score \n",
        "# -> IQR (Inter Quartile Range)\n",
        "# ZScore\n",
        "#importing stats to find the zscore\n",
        "zscore = np.abs(stats.zscore(df))\n",
        "\n",
        "#We cann't use ZScore method for finding and Removing Outliers as ZScore need only numerical value but \n",
        "#we have numerical as well as categorial values in dataset \n",
        "\n",
        "#So we will use IQR method for Removing Outliers\n",
        "\n",
        "#IQR\n",
        "df_iqr = df\n",
        "Q1 = df_iqr.quantile(0.25)\n",
        "Q3 = df_iqr.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print(IQR)\n",
        "\n",
        "#to check whether their is any outlier or not \n",
        "print(df_iqr < (Q1 - 1.5 * IQR)) | (df_iqr > (Q3 + 1.5 * IQR))\n",
        "\n",
        "#to remove the outliers\n",
        "df_iqr_clean = df_iqr[~((df_iqr < (Q1 - 1.5 * IQR)) | (df_iqr < (Q3 - 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "#to check the new value \n",
        "df_iqr_clean.shape\n",
        "\n",
        "#saving the clean data of IQR in Data1 file\n",
        "df_iqr_clean.to_csv('Data1.csv',index=False)\n",
        "\n",
        "#read the csv file and storing it in new file i.e df3\n",
        "df3 = pd.read_csv('/content/Data1.csv')\n",
        "df3.head(10)\n",
        "\n",
        "df3.shape\n",
        "\n",
        "#split the dataset in training and testing data in 70 and 30 ratio respectively and save it in the file\n",
        "#split the data into train and test set\n",
        "train,test = train_test_split(df3, test_size=0.30, random_state=0)\n",
        "#save the data\n",
        "train.to_csv('train1.csv',index=False)\n",
        "test.to_csv('test1.csv',index=False)\n",
        "\n",
        "#to read the train data from splited file of training data\n",
        "df_train = pd.read_csv('/content/train1.csv')\n",
        "df_train\n",
        "\n",
        "#to read the train data from splited file of training data\n",
        "df_test = pd.read_csv('/content/test1.csv')\n",
        "df_test\n",
        "\n",
        "#splitting independent and dependent variables of traning data\n",
        "x_train = pd.DataFrame(df_train.iloc[:,:-1])\n",
        "y_train = pd.DataFrame(df_train.iloc[:,-1])\n",
        "\n",
        "#splitting independent and dependent variables of training data\n",
        "x_test = pd.DataFrame(df_test.iloc[:,:-1])\n",
        "y_test = pd.DataFrame(df_test.iloc[:,-1])\n",
        "\n",
        "# CREATING PIPELINES\n",
        "\n",
        "#Logistic Regression Pipeline\n",
        "LogisticRegressionPipeline= Pipeline([('myscaler',MinMaxScaler()),\n",
        "                          ('mypca', PCA(n_components= 15)),\n",
        "                          ('logistic_classifier', LogisticRegression())])\n",
        "\n",
        "#Decision Tree Pipeline\n",
        "DecisionTreePipeline= Pipeline([('myscaler',MinMaxScaler()),\n",
        "                          ('mypca', PCA(n_components= 15)),\n",
        "                          ('DecisionTreeClassifier', DecisionTreeClassifier())])\n",
        "\n",
        "#Xgboost Pipeline\n",
        "XgboostPipeline= Pipeline([('myscaler',MinMaxScaler()),\n",
        "                          ('mypca', PCA(n_components= 15)),\n",
        "                          ('XgboostClassifier', XGBClassifier())])\n",
        "\n",
        "#Random Forest Classifier Pipeline\n",
        "RandomForestClassifierPipeline= Pipeline([('myscaler',MinMaxScaler()),\n",
        "                          ('mypca', PCA(n_components= 15)),\n",
        "                          ('RandomForestClassifier', RandomForestClassifier())])\n",
        "\n",
        "#Naive Baiyes Pipeline\n",
        "NaiveBaiyesPipeline= Pipeline([('myscaler',MinMaxScaler()),\n",
        "                          ('mypca', PCA(n_components= 15)),\n",
        "                          ('NaiveBaiyesClassifier', GaussianNB())])\n",
        "\n",
        "#Defining the pipeline in a list\n",
        "mypipeline = [LogisticRegressionPipeline, DecisionTreePipeline, XgboostPipeline, RandomForestClassifierPipeline, NaiveBaiyesPipeline]\n",
        "\n",
        "# Variables to choose best model\n",
        "accuracy = 0.0\n",
        "classifier = 0\n",
        "pipeline =\"\"\n",
        "\n",
        "#Creating dictionary of pipelines and training models\n",
        "PipelineDict = { 0: 'LogisticRegression', 1: 'DecisionTree', 2:'XgboostClassifier', 3: 'RandomForestClassifier', 4: 'NaiveBaiyesClassifier'}\n",
        "\n",
        "# fit the pipeline\n",
        "for mypipe in mypipeline :\n",
        "  mypipe.fit(x_train,y_train)\n",
        "\n",
        "# Getting Test Accuracy for all classifiers \n",
        "for i,model in enumerate(mypipeline):\n",
        "  print(\" {} Test Accuracy : {}\".format(PipelineDict[i], model.score(x_test,y_test)*100))\n",
        "\n",
        "# Choosing best model\n",
        "for i, model in enumerate(mypipeline):\n",
        "  if model.score(x_test,y_test)>accuracy:\n",
        "    accuracy = model.score(x_test,y_test)\n",
        "    pipeline = model\n",
        "    classifier = i\n",
        "print('Classifier with best accuracy :{}'.format(PipelineDict[classifier]))"
      ]
    }
  ]
}